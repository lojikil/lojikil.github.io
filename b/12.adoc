= No thoughts low thoughts generational thoughts: thoughts on No/Low Code vs Neural Network Code Gen

The title isn't a knock on no or low code, just a pithy title. And, not to bury the lede, but bottom line, up front:

The argument boils down to: what does a machine guess that we mean vs what does a restricted language allow us to express?
One can be theorized about and the other precisely understood. On the flip side, one has a huge corpus from which to build
code and can be tuned in its ability to understand both its source corpus and the question-target corpus, the other must be
programmed much like a traditional language, bugs and all. Also to further vouch my thoughts, I spend most of my days doing
code analysis; even my avocational time is spent there. So, I tend to err on “what can be understood,” vs “what can be generated.” 

== Prelude: why generate?

Originally, I wasn't going to talk about this aspect, but I've seen quite a few arguments *against* generation:

* the results are inscrutable
* the results are poor
* the process is used to drive down the need for programmers

As well as others. I think these are interesting comments and worth their own longer article, but I thought I could address
some of them here as well.

Two of the largest code bases I've ever reviewed were, at least in part, generated from some other source. The resulting code, whilst
human readable, was myriad, interwoven in ways that humans may not have intended, and difficult to conceptualize, let alone read. If
you consider low/no code to be a _tool_, this would be an unfortunate result: the code is not useful for use again, and often isn't human
readable. If you conisder low/no code to be a _compiler_ the results are more clear: you're setting yourself up on some platform, and the result
isn't something intended for human use. Honestly, as someone who has been working on compilers that output human-readable code for two decades,
I think with time we'll see better output on this front, especially if we request it. 

This also leads to the second point, that the results are poor. I tend to think of low code/no code, for the most part, as being like Access:
really great for initial proofs of concept, really terrible for longer term usage. Indeed, a few startups have started with no/low code, and
then overtime moved in part or in whole over to systems developed by programming experts. I tend to think of these systems as _force multipliers_:
they allow subject matter experts to focus on business logic, and programmers to focus on the hard problems that they face. Ideally, with input from
programmers and with time, the output of said tools will get better and be easier to integrate. Consider domain specific languages and their impact
on subject matter expertise, and how these ideas could be raised up to better expose business logic whilst allowing programmers to focus on what they
do best.

Lastly, I've been hearing how "any day now" we're not going to have any programmers left, that tools, outsourcing, whatever will replace the need for
programmers, and soon we'll be in this business utopia wherein we'll just make money from ideas and won't need to program directly ever again. It's
utter hogwash; it wasn't true then, it isn't true now, and likely won't be true for many years to come, if ever. We will always program, at some level,
even if our languages become visual and we have very little textual representation, we'll still need people who focus on logically representing ideas.
Tools such as specification & extraction engines are just another form of programming; maybe not one you like, but they *are still programming*. Look
at this https://concerningquality.com/semantics-extraction-isabelle/[article on extracting an interpreter from Isabelle/HOL]. This is a specification
that ends up generating code (in this case, OCaml), that the user themselves then runs. If you made slapped on a visual front end, named it something
intense like https://docs.appian.com/suite/help/22.3/sail/home.html[SAIL], and made SMT/constraint solving easier, it would be a product that you could
sell. What people aren't used to is that we always, intentionally or not, explicitly or implicitly, design our programs. Low code/no code, Machine
Learning, formal methods, &c. are all intended to _specify_ program functionality; they lay bare the domain & co-domain of the program that you
intend, and make it accessible to more people. If this were the 70s, prior to "AI Winter," we all would be ooh-ing and ahh-ing about how intelligent
the future looked; instead, we've become rightfully skeptical of the problems. But, we're still programming, we're still wrestling with the ideas,
and attempting to make it easier to do so.

So honestly, I'm not overly worried about CoPilot or SAIL or Isabelle eating my programmitic lunch or suddenly putting a whole class of developers
out of business. Instead, I'm interested in how we can make these systems transition better from the design phase and lower barrier of entry for
good business logic development interfacing with great programming systems. What I *do* want is better tools and systems, things like 
https://enso.org/[Enso (which used to be called Luna)], which is a mixed textual & visual language, or https://darklang.com/[Darklang], which 
mixes easier daily tasks with an ML or Haskell-style textual language and visual, domain specific languages and mixed representations, things that
delight and make specification easier to expose. Specification *is* programming, even if we're not manipulating registers one by one with `MOV` 
instructions.

== Basic Components, and intro

At it's core there are two real components that we're talking about here:

* a specifier that accepts human input in some form, such as via a Prompt for Machine Learning or an alternative syntax for No/Low code and
* a extractor (or generator) which acts upon human input to transpile this human intent into executable form.

Fundamentally, if we consider the problem of program extraction, these are the opposite ends of the same spectrum:

* Machine learning attempts to look at what the corpus of human code, divine intent, and produce something similar
* No/Low code looks at what humans have specified, and attempts to generate good or better code

In both cases, stronger models can lead to much better extraction: if we can divine user intent better with a huge corpus, we can more easily
recognize patterns of code that match what the user intends. On the other side, if we can more strongly encode user intent within some specification and
have strong generators for patterns of specifications to the extraction target, we can more easily extract code that tightly adheres to user intent.
Really, for both Machine Learning and No/Low Code:

* If we can better specify our problem (and understand the spec), we can use more information in our extractor
* If we can better extract our problem, we can get better transpiled results to the systems we actually want to use



.good note, should remove to another section
Really, I don't see either as much different from domain specific languages (DSLs): the intent is to solve a problem within a given problem domain,
in a syntax that makes sense to a subject matter expert. With Machine Learning especially, we're certainly broadening the field to being more accessible
to more people, but at the end of the day
                                                                                  


. at its core, there are two basical subsystems of components:
.. a specifier, which accepts human input
.. an extractor, which acts upon human input
. fundementally, these approaches are opposite ends of the same spectrum
.. Machine Learning attempts to look at what the corpus of code humans have produced is, and produce something similar
.. Low/No code looks at what you have specified and generates code as best it can
. in both, better models lead to better generators
.. if we can better specify our problem (and understand the spec), we can use more information in our extractor
.. if we can better extract our problem, we can better resolve issues
. additionally, both seek, in some way, to democritize programming
.. Machine Learning methods, by giving you the power of a huge corpus at your finger tips
.. Low/No Code, by giving you a sort of expert system
. Probably the best solution is a mix of both
.. But I prefer Low/No Code to Machine Learning

== Machine Learning methods

. Tensors & Vector spaces
. Talk about Vector Spaces of code
.. Salad is a good example
.. Maybe make a simple TF-IDF of code
. CoPilot and company work on *extremely deep* tensors with hundreds of features
. Human _understandable_ specification is the key

== (Semi-)Formal Modeling methods

. Various different methods
. Low vs No code
.. they're really both code, just what code they expose _to you_ is different
. Human _understandable_ generation is the key

== My Experiences

. talk about assessments with generated code from modeling tools
. talk about how we can do better

== Conclusions
